{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuniorHZ19/HerramientasIA/blob/main/srgan2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsgw-_4AS6fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *0) ** Instalando libreria(OBLIGATORIO)\n",
        "\n",
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "R0Je-JZAxuYY",
        "outputId": "9a2f3499-cd52-468c-c6d6-59da81b87b44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *1) ** Descargando DataSet  caras gatos(OBLIGATORIO)\n",
        "import zipfile\n",
        "\n",
        "nombre_zip=\"data_set_gatos.zip\"\n",
        "\n",
        "directorio_destino=\"/content/\"\n",
        "\n",
        "!gdown --id 1EK7H_QJqxDy0wBssMEi2_gWdo3zE2pmQ -O {nombre_zip}\n",
        "\n",
        "with zipfile.ZipFile(nombre_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directorio_destino)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxzaMyUeTGCC",
        "outputId": "b9c82307-9f3d-4a1e-9e39-39c2460e7253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EK7H_QJqxDy0wBssMEi2_gWdo3zE2pmQ\n",
            "To: /content/data_set_gatos.zip\n",
            "100% 284M/284M [00:03<00:00, 91.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *1) ** Clase Para manejo de directorios de datasets de imagenes\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "# Recorre el directorio  y elmiina los archvios que no tiene las extensioens permitidas\n",
        "\n",
        "class DataSetManage:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " def comprobar_ext_directorios(self,directorio):\n",
        "\n",
        "  for clase,[directorio,etiquetas] in(directorio.items()):\n",
        "   lista_directorio=os.listdir(directorio)\n",
        "   self._validarExt(directorio)\n",
        "\n",
        "\n",
        "\n",
        " def  _validarExt(self,directorio):\n",
        "  print(directorio)\n",
        "   # Extensiones permitidas\n",
        "  extensiones_permitidas = {\".jpg\", \".jpeg\", \".png\"}\n",
        "  for root, dirs, files in os.walk(directorio):\n",
        "\n",
        "    for file in files:\n",
        "        # Obtiene la extensión del archivo\n",
        "        _, extension = os.path.splitext(file)\n",
        "\n",
        "        # Verifica si la extensión no está en la lista de extensiones permitidas y elimina el archivo\n",
        "        if extension.lower() not in extensiones_permitidas:\n",
        "            archivo_a_eliminar = os.path.join(root, file)\n",
        "            os.remove(archivo_a_eliminar)\n",
        "            print(f\"Se eliminó: {archivo_a_eliminar}\")\n",
        "\n",
        "\n",
        "# Cambia nombre de cada archivo dentro del directorio a un valor secuencial\n",
        "\n",
        " def cambiar_nombres_directorios(self,directorio):\n",
        "   for clase,[directorio,etiquetas] in(directorio.items()):\n",
        "     lista_directorio=os.listdir(directorio)\n",
        "     self._cambiarNombre(directorio,clase)\n",
        "     print(directorio)\n",
        "\n",
        "\n",
        " def _cambiarNombre(self,directorios,subfijo):\n",
        "  archivos_en_directorio = os.listdir(directorios)\n",
        "  for i, archivo in enumerate(archivos_en_directorio, start=1):\n",
        "    # Construir el nuevo nombre del archivo\n",
        "    nuevo_nombre = f\"{subfijo}{i}{os.path.splitext(archivo)[1]}\"\n",
        "\n",
        "    # Ruta completa del archivo antiguo y nuevo\n",
        "    ruta_antigua = os.path.join(directorios, archivo)\n",
        "    ruta_nueva = os.path.join(directorios, nuevo_nombre)\n",
        "\n",
        "    # Cambiar el nombre del archivo\n",
        "    os.rename(ruta_antigua, ruta_nueva)\n",
        "    print(f\"Se cambió el nombre de {ruta_antigua} a {ruta_nueva}\")\n",
        "\n",
        "\n",
        "#Obtiene la cantidad de elemntos que tiene la carpeta\n",
        "\n",
        " def len_directorio(self,directorio):\n",
        "    cantidad_elementos = sum(1 for elemento in os.listdir(directorio) if os.path.isfile(os.path.join(directorio, elemento)))\n",
        "    return cantidad_elementos\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------\n",
        "\n",
        "#Valida si la imagen se puede leer usando  pill o cv2 si no se puede leer se elimina\n",
        "\n",
        " def validar_Img_Pill(self,directorio,):\n",
        "  for etiqueta,[directorio,clase] in(directorio.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self._validarLecturaImgPill(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        " def validar_Img_cv2(self,directorio):\n",
        "  for etiqueta,[directorio,clase] in(directorio.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self._validarLecturaImg(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        " def _validarLecturaImg(self,directorio,lista):\n",
        "\n",
        "  for ruta in(lista):\n",
        "   imagen=cv2.imread(directorio+ruta)\n",
        "   if  imagen is None:\n",
        "     os.remove(directorio+ruta)\n",
        "     print(f\"No se pudo leer y se elimino archivo:{directorio+ruta}\")\n",
        "\n",
        " def _validarLecturaImgPill(self,directorio,lista):\n",
        "    for ruta in(lista):\n",
        "     try:\n",
        "      imagen=Image.open(directorio+ruta)\n",
        "     except Exception as e:\n",
        "      os.remove(directorio+ruta)\n",
        "      print(f\"Archivo '{directorio+ruta}' eliminado.\")\n",
        "\n",
        "\n",
        "\n",
        "#vamos a recorrer el dicionario y validar ruta por ruta si se puede leer sino se elminara\n",
        "#vamos guaradno al mismo tiempo 3 listas, los directorios , listas de paths de los directiros y de las clases ,para usarlo luego usarlo al crear el csv\n",
        "\n",
        " def separar_datos_directorios(self,directorios):\n",
        "  listas_directorios=[]\n",
        "  listas_listas_directorios=[]\n",
        "  listas_clases=[]\n",
        "\n",
        "  for etiqueta,[directorio,clase] in(directorios.items()):\n",
        "\n",
        "       lista_paths=os.listdir(directorio)\n",
        "       listas_directorios.append(directorio)\n",
        "       listas_listas_directorios.append(lista_paths)\n",
        "       listas_clases.append(clase)\n",
        "\n",
        "  return listas_directorios,listas_listas_directorios,listas_clases\n",
        "\n",
        " def emparejar_listas_paths(self,lista_listas):\n",
        "\n",
        "   #Tomamos el minimo tamaño dentro de las lista de cada clase\n",
        "   tamaño_minimo = min(len(arr) for arr in lista_listas)\n",
        "\n",
        "   #Vamos a emparejar todas las listas con un tamaño igual que sea la del minimo tamaño de todas,esto para tener un set de datos parejo por cada clase\n",
        "   Reducido_lista_paths=[]\n",
        "\n",
        "   #Reduce cada lista de los paths a la cantidad minimo para que todos tenga iaugal cantidad\n",
        "   for listas in(lista_listas):\n",
        "    Reducido_lista_paths.append(listas[:tamaño_minimo])\n",
        "\n",
        "   return Reducido_lista_paths\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        " def crear_paths_csv(self,directorio_base,lista_paths,clases,nombre_archivo):\n",
        "\n",
        "   columnas=[\"path\",\"etiqueta\"]\n",
        "   datos_csv=[]\n",
        "\n",
        "   for dir_base,dir_path,clase in  zip(directorio_base,lista_paths,clases):\n",
        "\n",
        "    for path  in (dir_path):\n",
        "\n",
        "     datos_csv.append([dir_base+path ,clase])\n",
        "\n",
        "\n",
        "   df_lista=pd.DataFrame(datos_csv,columns=columnas)\n",
        "   df_lista.to_csv(nombre_archivo,index=False)\n",
        "   print(\"Csv Creado\")\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "#devuelve cuatnos elemtnos tiene cada clase\n",
        " def total_elementos(self,directorio,csv_path):\n",
        "    df=pd.read_csv(csv_path)\n",
        "    for clase,[directorio,etiqueta] in (directorio.items()):\n",
        "     tamaño_etiqueta=(df[\"etiqueta\"] == etiqueta).sum()\n",
        "     print(f\"la clase {clase} tiene :{tamaño_etiqueta} elementos\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "  #Funciones para data aumentation\n",
        " def   data_aumentation_conjunto(self,input_imagen_folder,output_path_folder,iteraciones,transformaciones):  #ingresa trnasformacioens como compose donde se aplicara las trasnfomracioens conjutnos pero se repteira un numero de veces por cada imagen\n",
        "\n",
        "   for filename in os.listdir(input_imagen_folder):\n",
        "\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "\n",
        "        input_path = os.path.join(input_imagen_folder, filename)\n",
        "        output_path = os.path.join(output_path_folder, f'transformed_{filename}')\n",
        "        # Aplica las transformaciones\n",
        "        print(output_path)\n",
        "        self._apply_transfomaciones_conjunto_it(input_path,output_path,int(iteraciones),transformaciones)\n",
        "\n",
        "\n",
        " def   data_aumentation_individual(self,input_imagen_folder,output_path_folder,transformaciones):  #las trnasfomaciones solo pasaremos la lista ya que ira aplicando la transformacion una por una por cada imagen\n",
        "\n",
        "  for filename in os.listdir(input_imagen_folder):\n",
        "\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        input_path = os.path.join(input_imagen_folder, filename)\n",
        "        output_path = os.path.join(output_path_folder, f'transformed_{filename}')\n",
        "\n",
        "        # Aplica las transformaciones\n",
        "        self._apply_transfomaciones_conjunto(input_path,output_path,transformaciones)\n",
        "\n",
        "\n",
        "\n",
        " def _apply_transfomaciones_conjunto_it(self,input_imagen_path,output_iamgen_path,iteraciones,transformations=None,): #aplica las transfomracioens  conjutas por iteracion y se guarda las iamgenes\n",
        "\n",
        "   imagen=Image.open(input_imagen_path)\n",
        "\n",
        "\n",
        "   for i in range(iteraciones):\n",
        "      imagen_trasformada=transformations(imagen)\n",
        "\n",
        "      out_root, out_extension = os.path.splitext(output_iamgen_path)\n",
        "      imagen_trasformada.save(f\"{out_root}_{i}{out_extension}\")\n",
        "\n",
        "\n",
        " def _apply_transfomaciones_conjunto(self,input_imagen_path,output_iamgen_path,transformations=None): #aplica las transfomracioens individuales  y se guarda las iamgenes\n",
        "\n",
        "  imagen=Image.open(input_imagen_path)\n",
        "\n",
        "  for i,transformacion in enumerate(transformations):\n",
        "   imagen_transformada=transformacion(imagen)\n",
        "   out_root, out_extension = os.path.splitext(output_iamgen_path)\n",
        "   imagen_transformada.save(f\"{out_root}_{i}{out_extension}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4Byd3D3TT7Ov"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos los arhcivo baja resolucion aparitr de imangenes alta resoulcion en hr , estos archios iran a carpaeta lw\n",
        "directorio_in=\"/content/hr/\"\n",
        "directorio_out=\"/content/lw/\"\n",
        "\n",
        "transformaciones=transforms.Compose([\n",
        "transforms.Resize((320,240)),\n",
        "])\n",
        "\n",
        "datasetmanage=DataSetManage()\n",
        "\n",
        "\n",
        "datasetmanage.data_aumentation_conjunto(directorio_in,directorio_out,1,transformaciones)"
      ],
      "metadata": {
        "id": "IGhbwh-jOnFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmg=DataSetManage()\n",
        "\n",
        "directoriolw={\n",
        "     \"lw\":[\"/content/lw/\",0],\n",
        "}\n",
        "\n",
        "directoriohr={\n",
        "     \"hr\":[\"/content/hr/\",1],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "listas_directorios,listas_paths_directorios,listas_clases=dmg.separar_datos_directorios(directoriolw)\n",
        "Dataset_csv=\"lw_dataset.csv\" #nombre que tenda nuestlo csv\n",
        "dmg.crear_paths_csv(listas_directorios,listas_paths_directorios,listas_clases,Dataset_csv)\n",
        "\n",
        "\n",
        "listas_directorios,listas_paths_directorios,listas_clases=dmg.separar_datos_directorios(directoriohr)\n",
        "Dataset_csv=\"hr_dataset.csv\" #nombre que tendla nuestlo csv\n",
        "dmg.crear_paths_csv(listas_directorios,listas_paths_directorios,listas_clases,Dataset_csv)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kzybet7nU6gH",
        "outputId": "f5ad641f-61fb-4da4-8e71-20c346870a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Csv Creado\n",
            "Csv Creado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *2) ** Creacion de clase DATASET(OBLIGATORIO)\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class MiDataSet(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file, transform=None):\n",
        "\n",
        "\n",
        "     self.data=pd.read_csv(csv_file)\n",
        "\n",
        "     self.x=self.data[\"path\"]\n",
        "     self.y=self.data[\"etiqueta\"]\n",
        "\n",
        "     self.transform=transform\n",
        "\n",
        "     self.samples=self.data[\"path\"].shape[0]\n",
        "\n",
        "  def __getitem__(self,id):\n",
        "\n",
        "    rut_imagen=self.x[id]\n",
        "    imagen=cv2.imread(rut_imagen)\n",
        "    etiqueta=self.y[id]\n",
        "\n",
        "\n",
        "    if imagen is None:\n",
        "      pillow_image = Image.open(rut_imagen)\n",
        "      numpy_image = np.array(pillow_image)\n",
        "      imagen=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      imagen_rgb = self.transform(imagen_rgb)\n",
        "\n",
        "    return imagen_rgb,etiqueta\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "   return self.samples\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vcmR_ZKhkGs"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "1WYVki2qv_EJ"
      },
      "outputs": [],
      "source": [
        "#@markdown # *0) ** Creando clase Discriminadora y Geneaadora:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator (nn.Module):\n",
        "  def __init__(self):\n",
        "   super(Discriminator,self).__init__()\n",
        "\n",
        "   self.conv1=nn.Conv2d(3,64,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv2=nn.Conv2d(64,64,kernel_size=3,stride=2, bias=False)\n",
        "\n",
        "   self.conv3=nn.Conv2d(64,128,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv4=nn.Conv2d(128,128,kernel_size=3,stride=2, bias=False)\n",
        "\n",
        "   self.conv5=nn.Conv2d(128,256,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv6=nn.Conv2d(256,256,kernel_size=3,stride=2, bias=False)\n",
        "\n",
        "   self.conv7=nn.Conv2d(256,512,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv8=nn.Conv2d(512,512,kernel_size=3,stride=2, bias=False)\n",
        "\n",
        "   self.flatt=nn.Flatten()\n",
        "\n",
        "   self.oculta1=nn.LazyLinear(1024)\n",
        "   self.salida= nn.LazyLinear(1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x=self.conv1(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv2(x)\n",
        "    x=nn.BatchNorm2d(64)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv3(x)\n",
        "    x=nn.BatchNorm2d(128)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv4(x)\n",
        "    x=nn.BatchNorm2d(128)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv5(x)\n",
        "    x=nn.BatchNorm2d(256)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv6(x)\n",
        "    x=nn.BatchNorm2d(256)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv7(x)\n",
        "    x=nn.BatchNorm2d(512)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv8(x)\n",
        "    x=nn.BatchNorm2d(512)(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.flatt(x)\n",
        "    x=self.oculta1(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.salida(x)\n",
        "    out=torch.sigmoid(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "import torch.nn.functional as F\n",
        "class Generador(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "   super(Generador,self).__init__()\n",
        "\n",
        "   self.convInicial=nn.Conv2d(3,64,kernel_size=9,stride=1)\n",
        "\n",
        "   self.convSalida1=nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "   self.convSalida2=nn.Conv2d(64,256,kernel_size=3,stride=1,padding=1)\n",
        "   self.convSalida3=nn.Conv2d(64,256,kernel_size=3,stride=1,padding=1)\n",
        "   self.pixelshuffel=nn.PixelShuffle(2)\n",
        "\n",
        "   self.convlast=nn.Conv2d(64,3,kernel_size=9,stride=1)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "\n",
        "    x=self.convInicial(x)\n",
        "    x_inicial=x\n",
        "    for i in range(5):\n",
        "     fx=self.block_residual(x)\n",
        "     x=fx+x\n",
        "     x= F.relu(x)\n",
        "\n",
        "\n",
        "    x= self.convSalida1(x)\n",
        "    x= nn.BatchNorm2d(64)(x)\n",
        "    x=x+x_inicial\n",
        "    x= F.relu(x)\n",
        "\n",
        "    x=self.convSalida2(x)\n",
        "    print(x.shape)\n",
        "    x=self.pixelshuffel(x)\n",
        "    print(x.shape)\n",
        "    x= F.relu(x)\n",
        "\n",
        "\n",
        "    x=self.convSalida3(x)\n",
        "    x=self.pixelshuffel(x)\n",
        "    x= F.relu(x)\n",
        "\n",
        "    out=self.convlast(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def block_residual(self,x):\n",
        "    f=nn.Sequential(nn.LazyConv2d(64,kernel_size=3,stride=1,padding=1),\n",
        "                  nn.BatchNorm2d(64),\n",
        "                  nn.ReLU(),\n",
        "                  nn.LazyConv2d(64,kernel_size=3,stride=1,padding=1),\n",
        "                  nn.BatchNorm2d(64),\n",
        "                    )\n",
        "\n",
        "    return f(x)\n"
      ],
      "metadata": {
        "id": "TuQmnT8vZej0"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generador=Generador()\n",
        "\n",
        "x=torch.randn(4,3,320,240)\n",
        "\n",
        "print(generador(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjlYRf2XYly3",
        "outputId": "8d008051-24b7-4a63-b856-04c9f13b6ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256, 312, 232])\n",
            "torch.Size([4, 64, 624, 464])\n",
            "torch.Size([4, 3, 1240, 920])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "import sklearn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "# ...\n",
        "\n",
        "#d=Discriminator(image_dim).to(device)\n",
        "#g=Generador(dim_vector_ruido,image_dim).to(device)\n",
        "\n",
        "#d.load_state_dict(torch.load(\"/content/modelo_gatos_d.pt\"))\n",
        "#g.load_state_dict(torch.load(\"/content/modelo_gatos_g.pt\"))\n",
        "\n",
        "#d=d.to(device)\n",
        "#g=g.to(device)\n",
        "\n",
        "\n",
        "# Supongamos que ya has definido las instancias de los modelos Generador (g) y Discriminador (d),\n",
        "# y has configurado los optimizadores (g_optimizer y d_optimizer) y la función de pérdida (criterio).\n",
        "\n",
        "\n",
        "\n",
        "def GAN(discriminador, generador, data_loader_lw,data_loader_hr, num_epochs, batch_size, criterio, d_optimizador, g_optimizador, device):\n",
        "    generador.to(device)\n",
        "    discriminador.to(device)\n",
        "    i=0\n",
        "    for epoch in range(num_epochs):\n",
        "        for  (lw, hr) in zip(data_loader_lw,data_loader_hr):\n",
        "\n",
        "            hr_img,hrlbl=hr\n",
        "            lw_img,lwlbl=lw\n",
        "\n",
        "            hr_img=hr_img.to(device)\n",
        "            lw_img=lw_img.to(device)\n",
        "            generador.eval()\n",
        "            discriminador.train()\n",
        "\n",
        "            real_images = hr_img.to(device)\n",
        "            real_labels = hrlbl.view(-1, 1).float().to(device)\n",
        "\n",
        "            # Entrenar el discriminador con imágenes reales\n",
        "            d_optimizador.zero_grad()\n",
        "            prediction_real = discriminador(real_images)\n",
        "            real_loss = criterio(prediction_real, real_labels)\n",
        "            real_loss.backward()\n",
        "\n",
        "            # Entrenar el discriminador con imágenes generadas\n",
        "\n",
        "            fake_images = generador(lw_img).detach()\n",
        "            fake_labels = torch.zeros(x.size(0), 1).to(device)\n",
        "\n",
        "            prediction_fake = discriminador(fake_images)\n",
        "            fake_loss = criterio(prediction_fake, fake_labels)\n",
        "            fake_loss.backward()\n",
        "\n",
        "            discriminator_loss = real_loss + fake_loss\n",
        "            d_optimizador.step()\n",
        "\n",
        "            # Entrenar el generador\n",
        "            generador.train()\n",
        "            discriminador.eval()\n",
        "\n",
        "            g_optimizador.zero_grad()\n",
        "            generated_images = generador(lw_img).to(device)\n",
        "            generator_loss = criterio(discriminador(generated_images), real_labels)\n",
        "            generator_loss.backward()\n",
        "            g_optimizador.step()\n",
        "            i=i+1\n",
        "            if i % 100 == 0:\n",
        "                print(f'Época [{epoch}/{num_epochs}], Paso [{i}/{len(data_loader_lw)}], '\n",
        "                      f'Pérdida del Discriminador: {discriminator_loss.item():.4f}, '\n",
        "                      f'Pérdida del Generador: {generator_loss.item():.4f}')\n",
        "\n",
        "        # Imprimir estadísticas y visualizar imágenes generadas al final de cada época\n",
        "        with torch.no_grad():\n",
        "            generador.eval()\n",
        "            generated_images = generador(lw_img).detach().cpu()\n",
        "\n",
        "            # Convertir las imágenes al rango [0, 1]\n",
        "            generated_images = (generated_images + 1) / 2.0\n",
        "\n",
        "            # Visualizar las imágenes generadas\n",
        "            plt.figure(figsize=(1, 1))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Imágenes Generadas - Época {epoch + 1}\")\n",
        "            plt.imshow(vutils.make_grid(generated_images, nrow=4).permute(1, 2, 0).numpy())\n",
        "            plt.show()\n",
        "#--------------------------------------------------------------------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA está disponible.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA no está disponible. Se utilizará la CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "batch_size=2\n",
        "num_epochs=50\n",
        "\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "\n",
        "])\n",
        "\n",
        "DatasetHr=MiDataSet(\"/content/hr_dataset.csv\",transformaciones)\n",
        "data_loaderHr=DataLoader(DatasetHr,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "DatasetLw=MiDataSet(\"/content/lw_dataset.csv\",transformaciones)\n",
        "data_loaderLw=DataLoader(DatasetLw,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "d=Discriminator().to(device)\n",
        "g=Generador().to(device)\n",
        "\n",
        "lr_g=0.0001\n",
        "lr_d=lr_g*1\n",
        "d_optimizer=optim.Adam(d.parameters(),lr=lr_d)\n",
        "g_optimizer=optim.Adam(g.parameters(),lr=lr_g)\n",
        "\n",
        "criterio=nn.BCELoss()\n",
        "print(f\"learning rate discrimiador:{lr_d}\")\n",
        "print(f\"learning rate generador:{lr_g}\")\n",
        "\n",
        "GAN(d,g,data_loaderLw,data_loaderHr,num_epochs,batch_size,criterio,d_optimizer,g_optimizer,device)\n",
        "\n"
      ],
      "metadata": {
        "id": "xOaEJmpkzNvo",
        "outputId": "5a414cf6-1f7b-4df3-aa4f-7231fc353279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA está disponible.\n",
            "learning rate discrimiador:0.0001\n",
            "learning rate generador:0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.95 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.23 GiB is free. Process 3558 has 13.52 GiB memory in use. Of the allocated memory 13.37 GiB is allocated by PyTorch, and 27.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-a2e0a1551f5b>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"learning rate generador:{lr_g}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loaderLw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loaderHr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-a2e0a1551f5b>\u001b[0m in \u001b[0;36mGAN\u001b[0;34m(discriminador, generador, data_loader_lw, data_loader_hr, num_epochs, batch_size, criterio, d_optimizador, g_optimizador, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Entrenar el discriminador con imágenes reales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0md_optimizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mprediction_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminador\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mreal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mreal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-055271db0da5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.95 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.23 GiB is free. Process 3558 has 13.52 GiB memory in use. Of the allocated memory 13.37 GiB is allocated by PyTorch, and 27.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g.state_dict(), 'modelo_gatos_g.pt')\n",
        "torch.save(d.state_dict(), 'modelo_gatos_d.pt')"
      ],
      "metadata": {
        "id": "nUbOcTLlt8m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que \"Generador\" es la clase de tu generador y \"ruta_modelo\" es la ruta del modelo guardado\n",
        "generador = Generador(100, 63*63*3)  # Ajusta input_size y output_size según tu implementación\n",
        "generador.load_state_dict(torch.load(\"modelo_gatos_g.pt\"))\n",
        "generador.to(device)\n",
        "generador.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    generador.eval()\n",
        "    noise = torch.randn(1, dim_vector_ruido).to(device)\n",
        "    generated_image = generador(noise).detach().cpu()\n",
        "\n",
        "# Convertir la imagen al rango [0, 1]\n",
        "generated_image = (generated_image + 1) / 2.0\n",
        "\n",
        "generated_image_np = generated_image.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "print(generated_image.shape)\n",
        "# Visualizar la imagen generada\n",
        "plt.imshow(generated_image_np)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k3ANH_wEuUBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}