{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuniorHZ19/HerramientasIA/blob/main/srgan5.7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
        "!unzip DIV2K_train_HR.zip # This is our dataset link. I will include this command in the description"
      ],
      "metadata": {
        "id": "hsgw-_4AS6fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *0) ** Instalando libreria(OBLIGATORIO)\n",
        "\n",
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "R0Je-JZAxuYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *1) ** Clase Para manejo de directorios de datasets de imagenes\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "# Recorre el directorio  y elmiina los archvios que no tiene las extensioens permitidas\n",
        "\n",
        "class DataSetManage:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " def comprobar_ext_directorios(self,directorio):\n",
        "\n",
        "  for clase,[directorio,etiquetas] in(directorio.items()):\n",
        "   lista_directorio=os.listdir(directorio)\n",
        "   self._validarExt(directorio)\n",
        "\n",
        "\n",
        "\n",
        " def  _validarExt(self,directorio):\n",
        "  print(directorio)\n",
        "   # Extensiones permitidas\n",
        "  extensiones_permitidas = {\".jpg\", \".jpeg\", \".png\"}\n",
        "  for root, dirs, files in os.walk(directorio):\n",
        "\n",
        "    for file in files:\n",
        "        # Obtiene la extensión del archivo\n",
        "        _, extension = os.path.splitext(file)\n",
        "\n",
        "        # Verifica si la extensión no está en la lista de extensiones permitidas y elimina el archivo\n",
        "        if extension.lower() not in extensiones_permitidas:\n",
        "            archivo_a_eliminar = os.path.join(root, file)\n",
        "            os.remove(archivo_a_eliminar)\n",
        "            print(f\"Se eliminó: {archivo_a_eliminar}\")\n",
        "\n",
        "\n",
        "# Cambia nombre de cada archivo dentro del directorio a un valor secuencial\n",
        "\n",
        " def cambiar_nombres_directorios(self,directorio):\n",
        "   for clase,[directorio,etiquetas] in(directorio.items()):\n",
        "     lista_directorio=os.listdir(directorio)\n",
        "     self._cambiarNombre(directorio,clase)\n",
        "     print(directorio)\n",
        "\n",
        "\n",
        " def _cambiarNombre(self,directorios,subfijo):\n",
        "  archivos_en_directorio = os.listdir(directorios)\n",
        "  for i, archivo in enumerate(archivos_en_directorio, start=1):\n",
        "    # Construir el nuevo nombre del archivo\n",
        "    nuevo_nombre = f\"{subfijo}{i}{os.path.splitext(archivo)[1]}\"\n",
        "\n",
        "    # Ruta completa del archivo antiguo y nuevo\n",
        "    ruta_antigua = os.path.join(directorios, archivo)\n",
        "    ruta_nueva = os.path.join(directorios, nuevo_nombre)\n",
        "\n",
        "    # Cambiar el nombre del archivo\n",
        "    os.rename(ruta_antigua, ruta_nueva)\n",
        "    print(f\"Se cambió el nombre de {ruta_antigua} a {ruta_nueva}\")\n",
        "\n",
        "\n",
        "#Obtiene la cantidad de elemntos que tiene la carpeta\n",
        "\n",
        " def len_directorio(self,directorio):\n",
        "    cantidad_elementos = sum(1 for elemento in os.listdir(directorio) if os.path.isfile(os.path.join(directorio, elemento)))\n",
        "    return cantidad_elementos\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------\n",
        "\n",
        "#Valida si la imagen se puede leer usando  pill o cv2 si no se puede leer se elimina\n",
        "\n",
        " def validar_Img_Pill(self,directorio,):\n",
        "  for etiqueta,[directorio,clase] in(directorio.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self._validarLecturaImgPill(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        " def validar_Img_cv2(self,directorio):\n",
        "  for etiqueta,[directorio,clase] in(directorio.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self._validarLecturaImg(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        " def _validarLecturaImg(self,directorio,lista):\n",
        "\n",
        "  for ruta in(lista):\n",
        "   imagen=cv2.imread(directorio+ruta)\n",
        "   if  imagen is None:\n",
        "     os.remove(directorio+ruta)\n",
        "     print(f\"No se pudo leer y se elimino archivo:{directorio+ruta}\")\n",
        "\n",
        " def _validarLecturaImgPill(self,directorio,lista):\n",
        "    for ruta in(lista):\n",
        "     try:\n",
        "      imagen=Image.open(directorio+ruta)\n",
        "     except Exception as e:\n",
        "      os.remove(directorio+ruta)\n",
        "      print(f\"Archivo '{directorio+ruta}' eliminado.\")\n",
        "\n",
        "\n",
        "\n",
        "#vamos a recorrer el dicionario y validar ruta por ruta si se puede leer sino se elminara\n",
        "#vamos guaradno al mismo tiempo 3 listas, los directorios , listas de paths de los directiros y de las clases ,para usarlo luego usarlo al crear el csv\n",
        "\n",
        " def separar_datos_directorios(self,directorios):\n",
        "  listas_directorios=[]\n",
        "  listas_listas_directorios=[]\n",
        "  listas_clases=[]\n",
        "\n",
        "  for etiqueta,[directorio,clase] in(directorios.items()):\n",
        "\n",
        "       lista_paths=os.listdir(directorio)\n",
        "       listas_directorios.append(directorio)\n",
        "       listas_listas_directorios.append(lista_paths)\n",
        "       listas_clases.append(clase)\n",
        "\n",
        "  return listas_directorios,listas_listas_directorios,listas_clases\n",
        "\n",
        " def emparejar_listas_paths(self,lista_listas):\n",
        "\n",
        "   #Tomamos el minimo tamaño dentro de las lista de cada clase\n",
        "   tamaño_minimo = min(len(arr) for arr in lista_listas)\n",
        "\n",
        "   #Vamos a emparejar todas las listas con un tamaño igual que sea la del minimo tamaño de todas,esto para tener un set de datos parejo por cada clase\n",
        "   Reducido_lista_paths=[]\n",
        "\n",
        "   #Reduce cada lista de los paths a la cantidad minimo para que todos tenga iaugal cantidad\n",
        "   for listas in(lista_listas):\n",
        "    Reducido_lista_paths.append(listas[:tamaño_minimo])\n",
        "\n",
        "   return Reducido_lista_paths\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        " def crear_paths_csv(self,directorio_base,lista_paths,clases,nombre_archivo):\n",
        "\n",
        "   columnas=[\"path\",\"etiqueta\"]\n",
        "   datos_csv=[]\n",
        "\n",
        "   for dir_base,dir_path,clase in  zip(directorio_base,lista_paths,clases):\n",
        "\n",
        "    for path  in (dir_path):\n",
        "\n",
        "     datos_csv.append([dir_base+path ,clase])\n",
        "\n",
        "\n",
        "   df_lista=pd.DataFrame(datos_csv,columns=columnas)\n",
        "   df_lista.to_csv(nombre_archivo,index=False)\n",
        "   print(\"Csv Creado\")\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "#devuelve cuatnos elemtnos tiene cada clase\n",
        " def total_elementos(self,directorio,csv_path):\n",
        "    df=pd.read_csv(csv_path)\n",
        "    for clase,[directorio,etiqueta] in (directorio.items()):\n",
        "     tamaño_etiqueta=(df[\"etiqueta\"] == etiqueta).sum()\n",
        "     print(f\"la clase {clase} tiene :{tamaño_etiqueta} elementos\")\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------\n",
        "\n",
        "  #Funciones para data aumentation\n",
        " def   data_aumentation_conjunto(self,input_imagen_folder,output_path_folder,iteraciones,transformaciones):  #ingresa trnasformacioens como compose donde se aplicara las trasnfomracioens conjutnos pero se repteira un numero de veces por cada imagen\n",
        "\n",
        "   for filename in os.listdir(input_imagen_folder):\n",
        "\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "\n",
        "        input_path = os.path.join(input_imagen_folder, filename)\n",
        "        output_path = os.path.join(output_path_folder, f'transformed_{filename}')\n",
        "        # Aplica las transformaciones\n",
        "        print(output_path)\n",
        "        self._apply_transfomaciones_conjunto_it(input_path,output_path,int(iteraciones),transformaciones)\n",
        "\n",
        "\n",
        " def   data_aumentation_individual(self,input_imagen_folder,output_path_folder,transformaciones):  #las trnasfomaciones solo pasaremos la lista ya que ira aplicando la transformacion una por una por cada imagen\n",
        "\n",
        "  for filename in os.listdir(input_imagen_folder):\n",
        "\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        input_path = os.path.join(input_imagen_folder, filename)\n",
        "        output_path = os.path.join(output_path_folder, f'transformed_{filename}')\n",
        "\n",
        "        # Aplica las transformaciones\n",
        "        self._apply_transfomaciones_conjunto(input_path,output_path,transformaciones)\n",
        "\n",
        "\n",
        "\n",
        " def _apply_transfomaciones_conjunto_it(self,input_imagen_path,output_iamgen_path,iteraciones,transformations=None,): #aplica las transfomracioens  conjutas por iteracion y se guarda las iamgenes\n",
        "\n",
        "   imagen=Image.open(input_imagen_path)\n",
        "\n",
        "\n",
        "   for i in range(iteraciones):\n",
        "      imagen_trasformada=transformations(imagen)\n",
        "\n",
        "      out_root, out_extension = os.path.splitext(output_iamgen_path)\n",
        "      imagen_trasformada.save(f\"{out_root}_{i}{out_extension}\")\n",
        "\n",
        "\n",
        " def _apply_transfomaciones_conjunto(self,input_imagen_path,output_iamgen_path,transformations=None): #aplica las transfomracioens individuales  y se guarda las iamgenes\n",
        "\n",
        "  imagen=Image.open(input_imagen_path)\n",
        "\n",
        "  for i,transformacion in enumerate(transformations):\n",
        "   imagen_transformada=transformacion(imagen)\n",
        "   out_root, out_extension = os.path.splitext(output_iamgen_path)\n",
        "   imagen_transformada.save(f\"{out_root}_{i}{out_extension}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4Byd3D3TT7Ov"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Creamos las carpetas con las imagenes en hr y lw\n",
        "\n",
        "#Creamos los arhcivo baja resolucion aparitr de imangenes alta resoulcion en hr , estos archios iran a carpaeta lw\n",
        "directorio_in=\"/content/DIV2K_train_HR\"\n",
        "directorio_out_lw=\"/content/lw/\"\n",
        "directorio_out_hr=\"/content/hr/\"\n",
        "\n",
        "#antes de otrogar mediads de hr y lw , chekea como el generador va votando las resolucioens hr_fake segun la resolucion lw que especifics tiene qeu conicidr\n",
        "#ya que el discriminador necsita la misma medidades tanto para el hr y el hr_fake que genera el generador\n",
        "\n",
        "transformaciones_hr=transforms.Compose([\n",
        "transforms.RandomCrop(size=(216, 216)),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "transformaciones_lw=transforms.Compose([\n",
        "transforms.Resize((60,60),interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "datasetmanage=DataSetManage()\n",
        "datasetmanage.data_aumentation_conjunto(directorio_in,directorio_out_hr,10,transformaciones_hr)\n",
        "datasetmanage.data_aumentation_conjunto(directorio_out_hr,directorio_out_lw,1,transformaciones_lw)\n"
      ],
      "metadata": {
        "id": "IGhbwh-jOnFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "directoriolw={\n",
        "     \"lw\":[\"/content/lw/\",0],\n",
        "}\n",
        "\n",
        "directoriohr={\n",
        "     \"hr\":[\"/content/hr/\",1],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "listas_directorios,listas_paths_directorios,listas_clases=dmg.separar_datos_directorios(directoriolw)\n",
        "Dataset_csv=\"lw_dataset.csv\" #nombre que tenda nuestlo csv\n",
        "dmg.crear_paths_csv(listas_directorios,listas_paths_directorios,listas_clases,Dataset_csv)\n",
        "\n",
        "\n",
        "listas_directorios,listas_paths_directorios,listas_clases=dmg.separar_datos_directorios(directoriohr)\n",
        "Dataset_csv=\"hr_dataset.csv\" #nombre que tendla nuestlo csv\n",
        "dmg.crear_paths_csv(listas_directorios,listas_paths_directorios,listas_clases,Dataset_csv)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kzybet7nU6gH",
        "outputId": "a562ad89-9c7b-476f-e26a-86257b1a6e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Csv Creado\n",
            "Csv Creado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFNIIR DISPOSITIVO:\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA está disponible.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA no está disponible. Se utilizará la CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "Q1hgFUL5RRgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *2) ** Creacion de clase DATASET(OBLIGATORIO)\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class MiDataSet(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file, transform=None):\n",
        "\n",
        "\n",
        "     self.data=pd.read_csv(csv_file)\n",
        "\n",
        "     self.x=self.data[\"path\"]\n",
        "     self.y=self.data[\"etiqueta\"]\n",
        "\n",
        "     self.transform=transform\n",
        "\n",
        "     self.samples=self.data[\"path\"].shape[0]\n",
        "\n",
        "  def __getitem__(self,id):\n",
        "\n",
        "    rut_imagen=self.x[id]\n",
        "    imagen=cv2.imread(rut_imagen)\n",
        "    etiqueta=self.y[id]\n",
        "\n",
        "\n",
        "    if imagen is None:\n",
        "      pillow_image = Image.open(rut_imagen)\n",
        "      numpy_image = np.array(pillow_image)\n",
        "      imagen=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      imagen_rgb = self.transform(imagen_rgb)\n",
        "\n",
        "    return imagen_rgb,etiqueta\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "   return self.samples\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vcmR_ZKhkGs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREANDO DATASETS Y DATALOADERS:\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "])\n",
        "\n",
        "batch_size=10\n",
        "DatasetHr=MiDataSet(\"/content/hr_dataset.csv\",transformaciones)\n",
        "data_loaderHr=DataLoader(DatasetHr,batch_size=batch_size)\n",
        "\n",
        "DatasetLw=MiDataSet(\"/content/lw_dataset.csv\",transformaciones)\n",
        "data_loaderLw=DataLoader(DatasetLw,batch_size=batch_size)\n",
        "batch_size=10\n",
        "DatasetHr=MiDataSet(\"/content/hr_dataset.csv\",transformaciones)\n",
        "data_loaderHr=DataLoader(DatasetHr,batch_size=batch_size)\n",
        "\n",
        "generated_image,etiqueta=DatasetHr.__getitem__(8)\n",
        "\n",
        "print(generated_image.shape)\n",
        "generated_image_np = generated_image.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "# Visualizar la imagen generada\n",
        "plt.imshow(generated_image_np)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nrIpUc77941u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1WYVki2qv_EJ"
      },
      "outputs": [],
      "source": [
        "#@markdown # *0) ** Creando clase Discriminadora y Geneaadora:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator (nn.Module):\n",
        "  def __init__(self):\n",
        "   super(Discriminator,self).__init__()\n",
        "\n",
        "   self.conv1=nn.Conv2d(3,64,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv2=nn.Conv2d(64,64,kernel_size=3,stride=2, bias=False)\n",
        "   self.btchnorm64=nn.BatchNorm2d(64)\n",
        "\n",
        "   self.conv3=nn.Conv2d(64,128,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv4=nn.Conv2d(128,128,kernel_size=3,stride=2, bias=False)\n",
        "   self.btchnorm128=nn.BatchNorm2d(128)\n",
        "\n",
        "   self.conv5=nn.Conv2d(128,256,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv6=nn.Conv2d(256,256,kernel_size=3,stride=2, bias=False)\n",
        "   self.btchnorm256=nn.BatchNorm2d(256)\n",
        "\n",
        "   self.conv7=nn.Conv2d(256,512,kernel_size=3,stride=1, bias=False)\n",
        "   self.conv8=nn.Conv2d(512,512,kernel_size=3,stride=2, bias=False)\n",
        "   self.btchnorm512=nn.BatchNorm2d(512)\n",
        "   self.flatt=nn.Flatten()\n",
        "\n",
        "   self.oculta1=nn.LazyLinear(1024)\n",
        "   self.salida= nn.LazyLinear(1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x=self.conv1(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv2(x)\n",
        "    x=self.btchnorm64(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv3(x)\n",
        "    x=self.btchnorm128(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv4(x)\n",
        "    x=self.btchnorm128(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv5(x)\n",
        "    x=self.btchnorm256(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv6(x)\n",
        "    x=self.btchnorm256(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv7(x)\n",
        "    x=self.btchnorm512(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv8(x)\n",
        "    x=self.btchnorm512(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.flatt(x)\n",
        "    x=self.oculta1(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.salida(x)\n",
        "    out=torch.sigmoid(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminador=Discriminator()\n",
        "discriminador=discriminador.to(device)\n",
        "x=torch.randn(20,3,208,208).to(device)\n",
        "\n",
        "print(discriminador(x))"
      ],
      "metadata": {
        "id": "UJ1Z0_7emJxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "import torch.nn.functional as F\n",
        "class Generador(nn.Module):\n",
        "\n",
        "  def __init__(self,factor_escala):\n",
        "   super(Generador,self).__init__()\n",
        "\n",
        "   self.convInicial=nn.Conv2d(3,64,kernel_size=9,stride=1,padding=4)\n",
        "\n",
        "  #Bloques residuales\n",
        "\n",
        "   res_blocks = []\n",
        "   for _ in range(5):\n",
        "       res_blocks.append(ResidualBlock(64))\n",
        "   self.res_blocks = nn.Sequential(*res_blocks)\n",
        "\n",
        "   self.convSalida1=nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1)\n",
        "   self.btchnorm=nn.BatchNorm2d(64)\n",
        "\n",
        "  #vamos a obligar que la salida anterior se ajuste a la entrada del bloque umsampling que es 256/factor al cuadrado, asi no tengamos problemas en el bluque\n",
        "\n",
        "   self.convextra=nn.Conv2d(64,int(256/factor_escala**2),kernel_size=1,stride=1)\n",
        "\n",
        "  #Bloques upsampling\n",
        "\n",
        "   upsampling = []\n",
        "   for out_features in range(2):\n",
        "        upsampling.append(upsampling_block(256,factor_escala))\n",
        "\n",
        "   self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "   self.prelu_layer=nn.PReLU();\n",
        "   self.convlast=nn.Conv2d(int(256/factor_escala**2),3,kernel_size=9,stride=1,padding=4)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    x=self.convInicial(x)\n",
        "    x= self.prelu_layer(x)\n",
        "    x_inicial=x\n",
        "\n",
        "    x=self.res_blocks(x)\n",
        "    x= self.convSalida1(x)\n",
        "\n",
        "    x=torch.add(x,x_inicial)\n",
        "    x=self.convextra(x)\n",
        "\n",
        "    x=self.upsampling(x)\n",
        "\n",
        "\n",
        "\n",
        "    x=self.convlast(x)\n",
        "    out= ((torch.tanh(x) + 1) / 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "class upsampling_block(nn.Module):\n",
        "    def __init__(self, out_features,factor):\n",
        "        super(upsampling_block, self).__init__()\n",
        "        print(int(out_features/factor**2))\n",
        "        self.conv_block = nn.Sequential(\n",
        "                nn.Conv2d(int(out_features/factor**2), out_features, 3, 1, 1),\n",
        "                nn.BatchNorm2d(out_features),\n",
        "                nn.PixelShuffle(upscale_factor=factor),\n",
        "                nn.PReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_block(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_features, 0.8),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n"
      ],
      "metadata": {
        "id": "TuQmnT8vZej0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VAMOS A VER QUE MEDIADES EL GENERADOR DA COMO RESUTLADO CON LW RESOLUTIONS MEDIDAES:\n",
        "\n",
        "generador=Generador(2)\n",
        "generador=generador.to(device)\n",
        "x=torch.randn(20,3,60,60).to(device)\n",
        "\n",
        "print(generador(x).shape)"
      ],
      "metadata": {
        "id": "Fyh-XN-zamlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PROBAMOS EL GENERADOR INCIAL:\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------\n",
        "def show_comparacion(real,generado): #deben de ingrenar don tennoren de imagen generada y real\n",
        " print(generado.shape)\n",
        " print(real.shape)\n",
        " real=real.detach().squeeze().cpu().numpy().transpose(1, 2, 0)\n",
        " generado=generado.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
        "# Primer subgráfico: imagen original\n",
        " plt.subplot(1, 2, 1)\n",
        " plt.imshow(generado)  # Convertir a NumPy y transponer dimensiones\n",
        " plt.title('Imagen Generada')\n",
        " plt.axis('off')\n",
        "\n",
        "# Segundo subgráfico: imagen transformada\n",
        " plt.subplot(1, 2, 2)\n",
        " plt.imshow(real)  # Convertir a NumPy y transponer dimensiones\n",
        " plt.title('Imagen Real')\n",
        " plt.axis('off')\n",
        "\n",
        "# Ajustar el diseño para evitar superposición de títulos\n",
        " plt.tight_layout()\n",
        "\n",
        "# Mostrar la figura\n",
        " plt.show()\n",
        "\n",
        "\n",
        " plt.figure(figsize=(10, 5))\n",
        "#-----------------------------------------------------------------------------------\n",
        "generador=Generador(2).to(\"cpu\")\n",
        "\n",
        "ruta_imagen = \"/content/lw/transformed_hr_8_0.png\"\n",
        "imagen = Image.open(ruta_imagen)\n",
        "\n",
        "transformador = ToTensor()\n",
        "\n",
        "#original iamgen tensor\n",
        "tensor_imagen = transformador(imagen).unsqueeze(0)\n",
        "tensor_imagen=tensor_imagen.to(\"cpu\")\n",
        "#imagen generada\n",
        "imagen_generada=generador(tensor_imagen)\n",
        "\n",
        "\n",
        "\n",
        "show_comparacion(tensor_imagen,imagen_generada)"
      ],
      "metadata": {
        "id": "UjlYRf2XYly3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "#vamos a extraer los feature es decir las caracteisicas imporantes que exiten entre ambas imagnes y compraralas esto se usara par luego hace rel loss contet apartir del contendio\n",
        "\n",
        "class Feature_Extractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Feature_Extractor, self).__init__()\n",
        "        # Cargar un modelo VGG-19 preentrenado\n",
        "        vgg19_model = models.vgg19(pretrained=True).features.eval()\n",
        "\n",
        "        # No actualizar los pesos del modelo VGG durante el entrenamiento\n",
        "        for param in vgg19_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        selected_layers = list(vgg19_model.children())[:18]\n",
        "\n",
        "        self.vgg=nn.Sequential(*selected_layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "\n",
        "        # Extraer características de las imágenes generada y real\n",
        "        feature_img = self.vgg(img)\n",
        "\n",
        "        return feature_img\n",
        "\n",
        "\n",
        "# TV Loss se utiliza comúnmente en tareas de generación de imágenes para reducir el ruido y fomentar la suavidad en las imágenes generadas.\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "  def __init__(self, tv_loss_weight=1):\n",
        "    super(TVLoss, self).__init__()\n",
        "    self.tv_loss_weight=tv_loss_weight\n",
        "  def forward(self, x):\n",
        "    batch_size=x.size()[0]\n",
        "    h_x = x.size()[2]\n",
        "    w_x = x.size()[3]\n",
        "\n",
        "    count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "    count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "\n",
        "\n",
        "    h_tv = torch.pow(x[:, :, 1:, :] - x[:, :, :h_x - 1, :], 2).sum()\n",
        "    w_tv = torch.pow(x[:, :, :, 1:] - x[:, :, :, :w_x - 1], 2).sum()\n",
        "    return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "  # Forgot to implement an important method\n",
        "  @staticmethod # Must add this\n",
        "  def tensor_size(t):\n",
        "    return t.size()[1] * t.size()[2] * t.size()[3]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cnWH1y8fEpTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_loss= TVLoss()\n",
        "x=torch.randn(18,3,60,60)\n",
        "tv_loss=tv_loss(x)\n",
        "\n",
        "print(tv_loss)"
      ],
      "metadata": {
        "id": "AIEYS7UZNql6",
        "outputId": "6b7ff54b-c7cc-4885-e1b6-02ad811debd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pasamos\n",
            "tensor(8.0288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "import sklearn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "# ...\n",
        "\n",
        "#d=Discriminator(image_dim).to(device)\n",
        "#g=Generador(dim_vector_ruido,image_dim).to(device)\n",
        "\n",
        "#d.load_state_dict(torch.load(\"/content/modelo_gatos_d.pt\"))\n",
        "#g.load_state_dict(torch.load(\"/content/modelo_gatos_g.pt\"))\n",
        "\n",
        "#d=d.to(device)\n",
        "#g=g.to(device)\n",
        "\n",
        "\n",
        "# Supongamos que ya has definido las instancias de los modelos Generador (g) y Discriminador (d),\n",
        "# y has configurado los optimizadores (g_optimizer y d_optimizer) y la función de pérdida (criterio).\n",
        "\n",
        "\n",
        "\n",
        "def GAN(discriminador, generador, data_loader_lw,data_loader_hr, num_epochs, batch_size, criterio_g,criterio_d, d_optimizador, g_optimizador, device):\n",
        "\n",
        "    discriminador=discriminador.to(device)\n",
        "    generador=generador.to(device)\n",
        "    feature_extractor=Feature_Extractor().to(device)\n",
        "    tv_loss= TVLoss().to(device)\n",
        "\n",
        "    i=0\n",
        "    for epoch in range(num_epochs):\n",
        "        for  (lw, hr) in zip(data_loader_lw,data_loader_hr):\n",
        "\n",
        "            hr_img,hrlbl=hr\n",
        "            lw_img,lwlbl=lw\n",
        "\n",
        "            hr_img=hr_img.to(device)\n",
        "            lw_img=lw_img.to(device)\n",
        "\n",
        "            generador.eval()\n",
        "            discriminador.train()\n",
        "\n",
        "            real_images = hr_img.float()\n",
        "\n",
        "            real_labels = hrlbl.view(-1, 1).float().to(device)\n",
        "\n",
        "            # Entrenar el discriminador con imágenes reales\n",
        "            d_optimizador.zero_grad()\n",
        "\n",
        "            prediction_real = discriminador(real_images)\n",
        "\n",
        "            real_loss = criterio_d(prediction_real, real_labels)\n",
        "\n",
        "\n",
        "            # Entrenar el discriminador con imágenes generadas\n",
        "\n",
        "            fake_images = generador(lw_img)\n",
        "            fake_labels = torch.zeros(lw_img.size(0), 1).to(device)\n",
        "\n",
        "            prediction_fake = discriminador(fake_images)\n",
        "\n",
        "            fake_loss = criterio_d(prediction_fake, fake_labels).to(device)\n",
        "            discriminator_loss = (real_loss + fake_loss)/2\n",
        "\n",
        "            discriminator_loss.backward(retain_graph=True)\n",
        "\n",
        "            d_optimizador.step()\n",
        "\n",
        "            # Entrenar el generador\n",
        "            generador.train()\n",
        "            discriminador.eval()\n",
        "            g_optimizador.zero_grad()\n",
        "\n",
        "            generated_images = generador(lw_img)\n",
        "            loss_g=criterio_g(discriminador(generated_images),real_labels) #loss del generador aparitr del disicmirnador que dice si es o no una imagen falsa\n",
        "\n",
        "            #loss_imagen=criterio_g(generated_images,real_images) #loss imagen dircamtetne entre igmagen genrada y real\n",
        "\n",
        "            generated_features=feature_extractor(generated_images)\n",
        "            real_features=feature_extractor(real_images)\n",
        "            content_loss=(criterio_g(generated_features, real_features)) #loss de los features original y generado,esto aparitr de el vgg19\n",
        "\n",
        "            tv_losss=tv_loss(generated_images)\n",
        "            generator_loss = content_loss+(0.001*loss_g)+ 2e-8 *tv_losss  #loss total del generador\n",
        "\n",
        "            torch.autograd.set_detect_anomaly(True)\n",
        "            generator_loss.backward()\n",
        "            g_optimizador.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Época [{epoch}/{num_epochs}], Paso [{i}/{len(data_loader_lw)}], '\n",
        "                      f'Pérdida del Discriminador: {discriminator_loss.item():.4f}, '\n",
        "                      f'Pérdida del Generador: {generator_loss.item():.4f}')\n",
        "            i=i+1\n",
        "        # Imprimir estadísticas y visualizar imágenes generadas al final de cada época\n",
        "        with torch.no_grad():\n",
        "            generador.eval()\n",
        "            generated_images = generador(lw_img)\n",
        "            print(hr_img.shape)\n",
        "            print(generated_images.shape)\n",
        "            show_comparacion(hr_img[0],generated_images[0])\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "num_epochs=80\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d=Discriminator()\n",
        "g=Generador(2)\n",
        "\n",
        "#d.load_state_dict(torch.load(\"/content/srgan_d.pt\"))\n",
        "#g.load_state_dict(torch.load(\"/content/srgan_g.pt\"))\n",
        "\n",
        "lr_g = 0.00008\n",
        "lr_d = 0.0000008\n",
        "d_optimizer=optim.Adam(d.parameters(),lr=lr_d)\n",
        "g_optimizer=optim.Adam(g.parameters(),lr=lr_g)\n",
        "\n",
        "criterio_d=nn.BCELoss()\n",
        "criterio_g=nn.MSELoss()\n",
        "\n",
        "\n",
        "print(f\"learning rate discrimiador:{lr_d}\")\n",
        "print(f\"learning rate generador:{lr_g}\")\n",
        "\n",
        "GAN(d,g,data_loaderLw,data_loaderHr,num_epochs,batch_size,criterio_g,criterio_d,d_optimizer,g_optimizer,device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xOaEJmpkzNvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g.state_dict(), 'srgan_g.pt')\n",
        "torch.save(d.state_dict(), 'srgan_d.pt')"
      ],
      "metadata": {
        "id": "nUbOcTLlt8m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.load_state_dict(torch.load(\"/content/srgan_d.pt\"))"
      ],
      "metadata": {
        "id": "7U056UsVyqpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Suponiendo que \"Generador\" es la clase de tu generador y \"ruta_modelo\" es la ruta del modelo guardado\n",
        "generador = Generador(2)  # creando generado\n",
        "generador.load_state_dict(torch.load(\"/content/srgan_g.pt\"))\n",
        "generador.to(device)\n",
        "generador.eval()\n",
        "\n",
        "ruta_imagen = \"/content/descarga (5).jpg\"\n",
        "imagen_pil = Image.open(ruta_imagen)\n",
        "\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convierte la imagen a un tensor\n",
        "\n",
        "])\n",
        "\n",
        "imagen_transformada = transformaciones(imagen_pil)\n",
        "imagen_transformada=imagen_transformada.to(device)\n",
        "with torch.no_grad():\n",
        "    generador.eval()\n",
        "\n",
        "    generated_image = generador(imagen_transformada.unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# Convertir la imagen al rango [0, 1]\n",
        "# generated_image = (generated_image + 1) / 2.0\n",
        "\n",
        "generated_image_np = generated_image.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "print(generated_image.shape)\n",
        "# Visualizar la imagen generada\n",
        "plt.imshow(generated_image_np)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Convertir el tensor a una imagen\n",
        "tensor_a_imagen = transforms.ToPILImage()(generated_image.squeeze())\n",
        "\n",
        "# Guardar la imagen\n",
        "tensor_a_imagen.save(\"imagen_guardadahd.jpg\")"
      ],
      "metadata": {
        "id": "k3ANH_wEuUBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}